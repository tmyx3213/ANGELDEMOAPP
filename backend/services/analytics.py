from __future__ import annotations
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd


def _safe_float(x):
    try:
        return float(x)
    except Exception:
        return None


def compute_profile(data: pd.DataFrame) -> Dict:
    # data: columns ds (datetime64), y (float)
    y = data['y'].astype(float)
    rows = int(len(data))
    date_min = data['ds'].min()
    date_max = data['ds'].max()
    mean = _safe_float(np.nanmean(y))
    median = _safe_float(np.nanmedian(y))
    vmin = _safe_float(np.nanmin(y))
    vmax = _safe_float(np.nanmax(y))
    std = _safe_float(np.nanstd(y, ddof=1) if rows > 1 else 0.0)
    cv = _safe_float((std / mean) if mean not in (None, 0) else None)
    # simple outlier count by IQR
    q1, q3 = np.nanpercentile(y, [25, 75]) if rows >= 4 else (None, None)
    iqr = (q3 - q1) if (q1 is not None and q3 is not None) else None
    lower = q1 - 1.5 * iqr if iqr is not None else None
    upper = q3 + 1.5 * iqr if iqr is not None else None
    outliers = int(((y < lower) | (y > upper)).sum()) if lower is not None else 0
    return {
        'rows': rows,
        'date_min': date_min.strftime('%Y-%m-%d') if pd.notna(date_min) else None,
        'date_max': date_max.strftime('%Y-%m-%d') if pd.notna(date_max) else None,
        'mean': mean, 'median': median, 'min': vmin, 'max': vmax,
        'std': std, 'cv': cv,
        'missing': 0, 'duplicates': 0, 'outliers': outliers,
    }


def compute_trend(data: pd.DataFrame) -> Dict:
    y = data['y'].astype(float).values
    n = len(y)
    # slope over last up to 30 points
    window = min(30, max(2, n))
    if n < 2:
        slope = 0.0
    else:
        yy = y[-window:]
        x = np.arange(len(yy), dtype=float)
        try:
            slope = float(np.polyfit(x, yy, 1)[0])
        except Exception:
            slope = 0.0
    # delta over ~3 months (~63 business days)
    lookback = min(n - 1, 63) if n > 1 else 0
    if lookback > 0 and y[-lookback-1] != 0:
        delta_3mo_pct = float((y[-1] - y[-lookback-1]) / y[-lookback-1] * 100)
    else:
        delta_3mo_pct = None
    return {
        'slope_30d': slope,
        'delta_3mo_pct': delta_3mo_pct,
        'changepoints': [],  # placeholder (optional)
    }


def compute_seasonality(data: pd.DataFrame) -> Dict:
    d = data.copy()
    d['weekday'] = d['ds'].dt.weekday  # Mon=0
    by_wd = d.groupby('weekday')['y'].mean()
    weekday_mean = by_wd.loc[[0,1,2,3,4]].mean() if set([0,1,2,3,4]).issubset(by_wd.index) else None
    weekend_mean = by_wd.loc[[5,6]].mean() if set([5,6]).issubset(by_wd.index) else None
    weekend_delta_pct = None
    if weekday_mean and weekend_mean:
        weekend_delta_pct = float((weekend_mean - weekday_mean) / weekday_mean * 100) if weekday_mean else None
    # crude acf at lag 7
    acf7 = None
    if len(d) > 14:
        y = d['y'].values.astype(float)
        y0 = y[:-7] - y[:-7].mean()
        y1 = y[7:] - y[7:].mean()
        denom = (np.linalg.norm(y0) * np.linalg.norm(y1))
        if denom != 0:
            acf7 = float(np.dot(y0, y1) / denom)
    # strength labels
    def label_from_val(val):
        if val is None:
            return 'ä¸æ˜'
        a = abs(val)
        if a >= 0.5:
            return 'å¼·'
        if a >= 0.2:
            return 'ä¸­'
        return 'å¼±'
    weekly_strength = label_from_val(acf7)
    return {
        'weekly_strength': weekly_strength,
        'weekend_delta_pct': weekend_delta_pct,
        'acf7': acf7,
    }


def summarize_forecast(data: pd.DataFrame, fc: pd.DataFrame, horizon_days: int) -> Dict:
    # future part is last horizon_days rows
    future = fc.tail(int(horizon_days)).copy() if len(fc) >= horizon_days else fc.copy()
    def pick(i):
        if len(future) == 0:
            return None, None, None
        idx = min(i-1, len(future)-1)
        row = future.iloc[idx]
        return float(row['yhat']), float(row['yhat_lower']), float(row['yhat_upper'])
    p50_5, lo_5, up_5 = pick(5)
    p50_30, lo_30, up_30 = pick(30)
    # band ratio median
    if len(future) > 0:
        ratio = (future['yhat_upper'] - future['yhat_lower']).abs() / future['yhat'].abs().replace(0, np.nan)
        band_ratio = float(np.nanmedian(ratio)) if ratio.notna().any() else None
    else:
        band_ratio = None
    # confidence
    def conf_from_band(r):
        if r is None:
            return 'ä¸æ˜'
        if r < 0.1:
            return 'é«˜'
        if r < 0.2:
            return 'ä¸­'
        return 'ä½'
    confidence = conf_from_band(band_ratio)

    latest = data['y'].iloc[-1] if len(data) else None
    delta_30_pct = float((p50_30 - latest) / latest * 100) if (latest not in (None, 0) and p50_30 is not None) else None

    return {
        'p50_5': p50_5, 'lo_5': lo_5, 'up_5': up_5,
        'p50_30': p50_30, 'lo_30': lo_30, 'up_30': up_30,
        'delta_30_pct': delta_30_pct,
        'band_ratio': band_ratio,
        'confidence': confidence,
    }


def generate_explanations(profile: Dict, trend: Dict, seas: Dict, fsum: Dict) -> Dict:
    # Business tone (multi-paragraph)
    p_rows = profile.get('rows')
    p_range = f"{profile.get('date_min')}ã€œ{profile.get('date_max')}"
    p_stats = f"å¹³å‡ {round(profile.get('mean', 0), 2)}ã€ä¸­å¤®å€¤ {round(profile.get('median', 0), 2)}ã€æœ€å° {round(profile.get('min', 0), 2)}ã€æœ€å¤§ {round(profile.get('max', 0), 2)}ã€‚æ¨™æº–åå·® {round(profile.get('std', 0), 2)}ã€å¤‰å‹•ä¿‚æ•° {round((profile.get('cv') or 0), 2)}ã€‚"
    p_quality = f"æ¬ æ {profile.get('missing')} ä»¶ã€é‡è¤‡ {profile.get('duplicates')} ä»¶ã€å¤–ã‚Œå€¤ {profile.get('outliers')} ä»¶ã€‚"
    s_line = f"é€±æ¬¡ã®å­£ç¯€æ€§ã¯{seas.get('weekly_strength')}ã§ã€é€±æœ«ã¯å¹³æ—¥æ¯” {round((seas.get('weekend_delta_pct') or 0), 1)}% ã§ã™ã€‚"
    t_line = f"ç›´è¿‘30æ—¥ã®å‚¾ãã¯ {round(trend.get('slope_30d') or 0, 4)}ã€ç›´è¿‘3ã‹æœˆã®å¤‰åŒ–ç‡ã¯ {round((trend.get('delta_3mo_pct') or 0), 1)}% ã§ã—ãŸã€‚"
    f_line = f"5æ—¥å…ˆã®äºˆæ¸¬ä¸­å¤®å€¤ã¯ {round((fsum.get('p50_5') or 0), 2)}ï¼ˆ{round((fsum.get('lo_5') or 0), 2)}ã€œ{round((fsum.get('up_5') or 0), 2)}ï¼‰ã€‚30æ—¥å…ˆã¯ {round((fsum.get('p50_30') or 0), 2)}ï¼ˆ{round((fsum.get('lo_30') or 0), 2)}ã€œ{round((fsum.get('up_30') or 0), 2)}ï¼‰ã§ã€æœ€æ–°å€¤æ¯” {round((fsum.get('delta_30_pct') or 0), 1)}% ã§ã™ã€‚"
    c_line = f"ä¿¡é ¼åº¦ã¯ {fsum.get('confidence')}ï¼ˆå¸¯å¹…æ¯” {round((fsum.get('band_ratio') or 0), 3)}ï¼‰ã€‚"

    business = "\n\n".join([
        f"ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ï¼šæœŸé–“ {p_range}ï¼ˆ{p_rows} ä»¶ï¼‰ã€‚",
        p_stats,
        p_quality,
        s_line,
        t_line,
        f_line,
        c_line,
    ])

    technical = "\n\n".join([
        f"åŸºæœ¬çµ±è¨ˆ: mean={round(profile.get('mean') or 0, 3)}, median={round(profile.get('median') or 0, 3)}, std={round(profile.get('std') or 0, 3)}, cv={round((profile.get('cv') or 0), 3)}",
        f"seasonality: acf7={round((seas.get('acf7') or 0), 3)}, weekend_delta_pct={round((seas.get('weekend_delta_pct') or 0), 2)}",
        f"trend: slope_30d={round((trend.get('slope_30d') or 0), 6)}, delta_3mo_pct={round((trend.get('delta_3mo_pct') or 0), 3)}",
        f"forecast: p50_5={fsum.get('p50_5')}, p50_30={fsum.get('p50_30')}, band_ratio={round((fsum.get('band_ratio') or 0), 4)}, confidence={fsum.get('confidence')}",
    ])

    # Rich narrative script for report mode (inspired by detailed report style)
    
    # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®åˆ¤å®š
    mean_val = profile.get('mean') or 0
    median_val = profile.get('median') or 0
    cv_val = profile.get('cv') or 0
    distribution_comment = "æ¯”è¼ƒçš„å¯¾ç§°çš„ãªåˆ†å¸ƒ" if abs(mean_val - median_val) / max(mean_val, 1) < 0.1 else "ã‚„ã‚„åã‚Šã®ã‚ã‚‹åˆ†å¸ƒ"
    variation_level = "å®‰å®šçš„" if cv_val < 0.15 else "ã‚„ã‚„å¤‰å‹•ãŒå¤§ãã„" if cv_val < 0.3 else "å¤‰å‹•ã®å¤§ãã„"
    
    weekend_pct = seas.get('weekend_delta_pct') or 0
    seasonality_strength = seas.get('weekly_strength', 'ä¸æ˜')
    seasonality_comment = f"æ˜ç¢ºãªé€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé€±æœ« +{abs(weekend_pct):.1f}%ï¼‰" if seasonality_strength == 'å¼·' else f"é€±æ¬¡ã®å¤‰å‹•ï¼ˆ{weekend_pct:+.1f}%ï¼‰" if seasonality_strength == 'ä¸­' else "å­£ç¯€æ€§ã¯é™å®šçš„"
    
    delta_3mo = trend.get('delta_3mo_pct') or 0
    trend_direction = "ä¸Šæ˜‡å‚¾å‘" if delta_3mo > 2 else "ä¸‹é™å‚¾å‘" if delta_3mo < -2 else "æ¨ªã°ã„å‚¾å‘"
    
    forecast_confidence = fsum.get('confidence', 'ä¸æ˜')
    prediction_reliability = "é«˜ã„ç²¾åº¦" if forecast_confidence == 'é«˜' else "ä¸­ç¨‹åº¦ã®ç²¾åº¦" if forecast_confidence == 'ä¸­' else "é™å®šçš„ãªç²¾åº¦"
    
    script = [
        {
            "id": "opening", 
            "text": f"âœ¨ åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚ä»Šå›æ‰±ã£ãŸã®ã¯{p_range}ã®{p_rows}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚å…¨ä½“ã‚’æ¦‚è¦³ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚", 
            "highlight": ["profile.rows"], 
            "waitMs": 4000
        },
        {
            "id": "data_overview", 
            "text": f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´ã‚’è¦‹ã‚‹ã¨ã€å¹³å‡{mean_val:.1f}ã€ä¸­å¤®å€¤{median_val:.1f}ã§ã€{distribution_comment}ã«ãªã£ã¦ã„ã¾ã™ã€‚", 
            "highlight": ["profile.mean", "profile.median"], 
            "waitMs": 4500
        },
        {
            "id": "variation_analysis", 
            "text": f"å¤‰å‹•ä¿‚æ•°ã¯{cv_val:.2f}ã§ã€çµ±è¨ˆçš„ã«ã¯ã€Œ{variation_level}ã€ãƒ‡ãƒ¼ã‚¿ã¨ã„ãˆã¾ã™ã€‚ã“ã‚Œã¯äºˆæ¸¬ã®ä¿¡é ¼æ€§ã«ã‚‚å½±éŸ¿ã—ã¾ã™ã€‚", 
            "highlight": ["profile.cv"], 
            "waitMs": 5000
        },
        {
            "id": "seasonality_analysis", 
            "text": f"ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã§ã¯{seasonality_comment}ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®é‡è¦ãªæ‰‹ãŒã‹ã‚Šã¨ãªã‚Šã¾ã™ã€‚", 
            "highlight": ["seasonality.weekly_strength"], 
            "waitMs": 5500
        },
        {
            "id": "trend_analysis", 
            "text": f"ğŸ“ˆ é•·æœŸçš„ã«ã¯{trend_direction}ã§ã€ç›´è¿‘3ãƒ¶æœˆã§ã¯{delta_3mo:+.1f}%ã®å¤‰åŒ–ã¨ãªã£ã¦ã„ã¾ã™ã€‚", 
            "highlight": ["trend.delta_3mo_pct"], 
            "waitMs": 4500
        },
        {
            "id": "forecast_results", 
            "text": f"ğŸ”® äºˆæ¸¬çµæœã¨ã—ã¦ã€30æ—¥å…ˆã®ä¸­å¤®å€¤ã¯{fsum.get('p50_30', 0):.1f}ã§ã€æœ€æ–°å€¤ã‹ã‚‰{fsum.get('delta_30_pct', 0):+.1f}%ã®å¤‰åŒ–ãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚", 
            "highlight": ["forecast.p50_30", "forecast.delta_30_pct"], 
            "waitMs": 6000
        },
        {
            "id": "reliability_assessment", 
            "text": f"âš–ï¸ ä»Šå›ã®äºˆæ¸¬ã¯{prediction_reliability}ã§ã®çµæœã¨ãªã£ã¦ãŠã‚Šã€å®Ÿç”¨çš„ãªè¦‹é€šã—ã¨ã—ã¦æ´»ç”¨ã§ãã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚", 
            "highlight": ["forecast.confidence"], 
            "waitMs": 5000
        },
        {
            "id": "conclusion", 
            "text": f"ğŸ“ ä»¥ä¸Šã®åˆ†æã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è¸ã¾ãˆãŸå°†æ¥äºˆæ¸¬ã‚’ãŠå±Šã‘ã—ã¾ã—ãŸã€‚è©³ç´°ã¯å³ä¸Šã®ã€Œè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã€ã‚‚ã”å‚ç…§ãã ã•ã„ã€‚", 
            "highlight": [], 
            "waitMs": 5500
        }
    ]

    targets = {
        # keys map to data-metric on frontend components
        'profile.rows': ["[data-metric='profile.rows']"],
        'profile.range': ["[data-metric='profile.range']"],
        'profile.mean': ["[data-metric='profile.mean']"],
        'profile.median': ["[data-metric='profile.median']"],
        'profile.std': ["[data-metric='profile.std']"],
        'profile.cv': ["[data-metric='profile.cv']"],
        'seasonality.weekly_strength': ["[data-metric='seasonality.weekly_strength']"],
        'seasonality.weekend_delta_pct': ["[data-metric='seasonality.weekend_delta_pct']"],
        'trend.slope_30d': ["[data-metric='trend.slope_30d']"],
        'trend.delta_3mo_pct': ["[data-metric='trend.delta_3mo_pct']"],
        'forecast.p50_5': ["[data-metric='forecast.p50_5']"],
        'forecast.p50_30': ["[data-metric='forecast.p50_30']"],
        'forecast.delta_30_pct': ["[data-metric='forecast.delta_30_pct']"],
        'forecast.confidence': ["[data-metric='forecast.confidence']"],
        'forecast.band_ratio': ["[data-metric='forecast.band_ratio']"],
    }

    return {
        'business': business,
        'technical': technical,
        'narrativeScript': script,
        'targets': targets,
    }

